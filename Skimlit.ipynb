{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3sayscr--i8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
        "!ls pubmed-rct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fJ1IaDB_Uuc"
      },
      "outputs": [],
      "source": [
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD0rXuMm_kNP"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
        "\n",
        "import os\n",
        "filename = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCTXBSTk_3yy"
      },
      "outputs": [],
      "source": [
        "def get_line(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "  return lines\n",
        "\n",
        "def preprocess_text_with_line_number(filename):\n",
        "  input_line = get_line(filename)\n",
        "  abstract_line = \"\"\n",
        "  abstract_samples = []\n",
        "\n",
        "  for line in input_line:\n",
        "    if line.startswith('###'):\n",
        "      abstract_id = line\n",
        "      abstract_lines = \"\"\n",
        "    elif line.isspace():\n",
        "      abstract_line_split = abstract_line.splitlines()\n",
        "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "        line_data = {}\n",
        "        target_text_split = abstract_line.split(\"\\t\")\n",
        "        line_data[\"target\"] = target_text_split[0]\n",
        "        line_data[\"text\"] = target_text_split[1].lower()\n",
        "        line_data[\"line_number\"] = abstract_line_number\n",
        "        line_data[\"total_lines\"] = len(abstract_line_split) - 1\n",
        "        abstract_samples.append(line_data)\n",
        "\n",
        "    else:\n",
        "      abstract_line += line\n",
        "\n",
        "  return abstract_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbFyDlsJALhr"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "train_samples = preprocess_text_with_line_number(data_dir+\"train.txt\")\n",
        "val_samples = preprocess_text_with_line_number(data_dir+\"dev.txt\")\n",
        "test_samples = preprocess_text_with_line_number(data_dir+\"test.txt\")\n",
        "len(train_samples), len(val_samples), len(test_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-G5N4RkAd7-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQung7VuAn1r"
      },
      "outputs": [],
      "source": [
        "train_sentences = train_df.text.tolist()\n",
        "val_sentences = val_df.text.tolist()\n",
        "test_sentences = test_df.text.tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUBjwHMdA0eg"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "train_labels_one_hot = ohe.fit_transform((train_df[[\"target\"]]).to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = ohe.transform((val_df[[\"target\"]]).to_numpy().reshape(-1, 1))\n",
        "test_labels_one_hot = ohe.transform((test_df[[\"target\"]]).to_numpy().reshape(-1, 1))\n",
        "train_labels_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ohe.categories_[0]\n",
        "class_names"
      ],
      "metadata": {
        "id": "nj_cMAxdMnSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icTkxPhTA-um"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2kr39CYBHFE"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "train_labels_encoded = le.fit_transform((train_df[\"target\"]).to_numpy())\n",
        "val_labels_encoded = le.transform((val_df[\"target\"]).to_numpy())\n",
        "test_labels_encoded = le.transform((test_df[\"target\"]).to_numpy())\n",
        "train_labels_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKFPgZS1BKKC"
      },
      "outputs": [],
      "source": [
        "sen_lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "output_seq_len = int(np.percentile(sen_lens, 95))\n",
        "max_seq_len = max(sen_lens)\n",
        "avg_sent = np.mean(sen_lens)\n",
        "max_seq_len, output_seq_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4d1mLIvIjN3"
      },
      "outputs": [],
      "source": [
        "max_token = 68000\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_token,\n",
        "                                    output_sequence_length=output_seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz6tZ3NxIxJm"
      },
      "outputs": [],
      "source": [
        "text_vectorizer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uapq1AsuI_Rr"
      },
      "outputs": [],
      "source": [
        "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
        "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab),\n",
        "                               output_dim=128,\n",
        "                               mask_zero=True,\n",
        "                               name=\"token_embedding\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N95tt7oJU5b"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
        "\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEJ94mHmJyC_"
      },
      "outputs": [],
      "source": [
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = token_embed(x)\n",
        "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "model_1.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHBY-iaLK1aC"
      },
      "outputs": [],
      "source": [
        "#model_1.fit(train_dataset, epochs=5, validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiMhyRfvLkzd"
      },
      "outputs": [],
      "source": [
        "# Pre-compute embeddings for training, validation, and test sentences\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "# Define the input layer for numerical embeddings\n",
        "inputs = layers.Input(shape=(512,), dtype=tf.float32) # Changed shape to (512,) and dtype to float32\n",
        "# Define a function to create embeddings in batches to avoid memory issues\n",
        "def create_use_embeddings(sentences, batch_size=32):\n",
        "    embeddings = []\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(sentences).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    with tf.device('/cpu:0'): # Ensure embedding happens on CPU\n",
        "        for batch in dataset:\n",
        "            embeddings.append(embed(batch).numpy())\n",
        "    return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "train_embeddings = create_use_embeddings(train_sentences)\n",
        "val_embeddings = create_use_embeddings(val_sentences)\n",
        "test_embeddings = create_use_embeddings(test_sentences)\n",
        "\n",
        "print(f\"Shape of train_embeddings: {train_embeddings.shape}\")\n",
        "print(f\"Shape of val_embeddings: {val_embeddings.shape}\")\n",
        "print(f\"Shape of test_embeddings: {test_embeddings.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "# The rest of the model layers\n",
        "x = layers.Dense(128, activation=\"relu\")(inputs) # Removed the USEEmbeddingLayer\n",
        "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
        "model_2 = tf.keras.Model(inputs, outputs)\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKO4llETMGy0"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Train model_2 on pre-computed embeddings\n",
        "history_model_2 = model_2.fit(train_embeddings, train_labels_one_hot,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_embeddings, val_labels_one_hot))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEFRtom_MN-3"
      },
      "outputs": [],
      "source": [
        "#model_1.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXlCvm-dnUXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFXR9aLKMihX"
      },
      "outputs": [],
      "source": [
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = token_embed(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset,\n",
        "            epochs=5,\n",
        "            validation_data=val_dataset)"
      ],
      "metadata": {
        "id": "yzmiWShLnjlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "Q4HqJVyypfWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb5OujTxMjM-"
      },
      "outputs": [],
      "source": [
        "train_line_one_hot = tf.one_hot(train_df.line_number.to_numpy(), depth=15)\n",
        "val_line_one_hot = tf.one_hot(val_df.line_number.to_numpy(), depth=15)\n",
        "test_line_one_hot = tf.one_hot(test_df.line_number.to_numpy(), depth=15)\n",
        "\n",
        "train_total_lines_one_hot = tf.one_hot(train_df.total_lines.to_numpy(), depth=20)\n",
        "val_total_lines_one_hot = tf.one_hot(val_df.total_lines.to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df.total_lines.to_numpy(), depth=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfRXHCvpMmjG"
      },
      "outputs": [],
      "source": [
        "# Define inputs for each branch\n",
        "token_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"token_inputs_embedding\")\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype=tf.float32, name=\"line_number_inputs\") # Corrected dtype and shape\n",
        "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=\"total_number_inputs\") # Corrected dtype and shape\n",
        "\n",
        "\n",
        "token_x = text_vectorizer(token_inputs)\n",
        "token_x = token_embed(token_x)\n",
        "token_x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_x)\n",
        "token_x = layers.GlobalMaxPool1D()(token_x)\n",
        "token_outputs = layers.Dense(5, activation=\"softmax\")(token_x)\n",
        "model = tf.keras.Model(token_inputs, token_outputs)\n",
        "\n",
        "# Token branch\n",
        "x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(64)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Line number branch: Dense layer\n",
        "line_number_output = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "\n",
        "# Total lines branch: Dense layer\n",
        "total_lines_output = layers.Dense(32, activation='relu')(total_lines_inputs)\n",
        "\n",
        "# Combine combined embeddings with positional embeddings\n",
        "tribrid_embeddings = layers.Concatenate(name=\"char_token_positional_embedding\")([line_number_output, total_lines_output, model.output])\n",
        "\n",
        "# Output layer\n",
        "output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(tribrid_embeddings)\n",
        "\n",
        "# Create the final model\n",
        "model_5 = tf.keras.Model(inputs=[line_number_inputs, total_lines_inputs, token_inputs],\n",
        "                         outputs=output_layer)\n",
        "\n",
        "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztInV55bMj0Q"
      },
      "outputs": [],
      "source": [
        "# Create training dataset for model_5\n",
        "train_dataset_complex = tf.data.Dataset.from_tensor_slices((train_line_one_hot,\n",
        "                                                            train_total_lines_one_hot,\n",
        "                                                            train_sentences)) # Changed train_sentences to vectorized sentences\n",
        "train_dataset_clabels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
        "\n",
        "train_dataset_model_5 = tf.data.Dataset.zip((train_dataset_complex, train_dataset_clabels))\n",
        "\n",
        "# Create validation dataset for model_5\n",
        "val_dataset_complex = tf.data.Dataset.from_tensor_slices((val_line_one_hot,\n",
        "                                                          val_total_lines_one_hot,\n",
        "                                                          val_sentences)) # Changed val_sentences to vectorized sentencesA\n",
        "\n",
        "v_dataset_clabels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot) # Changed train_labels_one_hot to val_labels_one_hot\n",
        "\n",
        "val_dataset_model_5 = tf.data.Dataset.zip((val_dataset_complex, v_dataset_clabels))\n",
        "\n",
        "# Batch and prefetch the datasets\n",
        "train_dataset_model_5 = train_dataset_model_5.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset_model_5 = val_dataset_model_5.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Datasets for model_5 created:\")\n",
        "print(train_dataset_model_5.element_spec)\n",
        "print(val_dataset_model_5.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5.fit(train_dataset_model_5, epochs=5, validation_data=val_dataset_model_5)"
      ],
      "metadata": {
        "id": "7YhDsCEAxaZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_complex = tf.data.Dataset.from_tensor_slices((test_line_one_hot,\n",
        "                                                           test_total_lines_one_hot,\n",
        "                                                           test_sentences)) # Changed test_sentences to vectorized sentences\n",
        "test_dataset_clabels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot) # Changed train_labels_one_hot to test_labels_one_hot\n",
        "test_dataset_model_5 = tf.data.Dataset.zip((test_dataset_complex, test_dataset_clabels))\n",
        "test_dataset_model_5 = test_dataset_model_5.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "model_5.evaluate(test_dataset_model_5)"
      ],
      "metadata": {
        "id": "HOXhnu6OxqGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define inputs for each branch\n",
        "token_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"token_inputs_embedding\")\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype=tf.float32, name=\"line_number_inputs\") # Corrected dtype and shape\n",
        "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=\"total_number_inputs\") # Corrected dtype and shape\n",
        "\n",
        "\n",
        "token_x = text_vectorizer(token_inputs)\n",
        "token_x = token_embed(token_x)\n",
        "token_x = layers.Dense(64, activation=\"relu\")(token_x)\n",
        "token_x = layers.GlobalMaxPool1D()(token_x)\n",
        "token_outputs = layers.Dense(5, activation=\"softmax\")(token_x)\n",
        "model = tf.keras.Model(token_inputs, token_outputs)\n",
        "\n",
        "# Token branch\n",
        "x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(64)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Line number branch: Dense layer\n",
        "line_number_output = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "\n",
        "# Total lines branch: Dense layer\n",
        "total_lines_output = layers.Dense(32, activation='relu')(total_lines_inputs)\n",
        "\n",
        "# Combine combined embeddings with positional embeddings\n",
        "tribrid_embeddings = layers.Concatenate(name=\"char_token_positional_embedding\")([line_number_output, total_lines_output, model.output])\n",
        "\n",
        "# Output layer\n",
        "output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(tribrid_embeddings)\n",
        "\n",
        "# Create the final model\n",
        "model_4 = tf.keras.Model(inputs=[line_number_inputs, total_lines_inputs, token_inputs],\n",
        "                         outputs=output_layer)\n",
        "\n",
        "model_4.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "I1Ihx0Ex0KN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.fit(train_dataset_model_5, epochs=5, validation_data=val_dataset_model_5)"
      ],
      "metadata": {
        "id": "v-JpcayI0QaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.evaluate(test_dataset_model_5)"
      ],
      "metadata": {
        "id": "q9uqUJYGTZ9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a use case for my model"
      ],
      "metadata": {
        "id": "o-UizUZf079u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I818RQyh1FUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract = \"Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM—a model that achieves an optimized tradeoff between resolution, latency, and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA1.5 setup, FastVLM achieves 3.2× improvement in time-tofirst-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152×1152), FastVLM achieves better performance on key benchmarks like SeedBench, MMMU and DocVQA, using the same 0.5B LLM, but with 85× faster TTFT and a vision encoder that is 3.4× smaller. Code and models are availab\""
      ],
      "metadata": {
        "id": "eSXktzxc24pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentence(text):\n",
        "  sentences = text.split(\"**.\")\n",
        "  return sentences\n",
        "\n",
        "def preprocess_text(text):\n",
        "  input_line = split_sentence(text)\n",
        "  total_lines = len(input_line)\n",
        "  df = pd.DataFrame({\n",
        "    'line_number': range(total_lines), # Start from 0\n",
        "    'total_lines': total_lines - 1, # Total lines is 0-indexed\n",
        "    'text': input_line\n",
        "})\n",
        "\n",
        "  text_line_one_hot = tf.one_hot(df.line_number.to_numpy(), depth=15)\n",
        "  text_total_lines_one_hot = tf.one_hot(df.total_lines.to_numpy(), depth=20)\n",
        "  text_sentences = df.text.tolist()\n",
        "\n",
        "  # Create a dataset where each element is a dictionary of the three inputs\n",
        "  text_dataset_complex = tf.data.Dataset.from_tensor_slices({\n",
        "      \"line_number_inputs\": text_line_one_hot,\n",
        "      \"total_number_inputs\": text_total_lines_one_hot,\n",
        "      \"token_inputs_embedding\": tf.constant(text_sentences, dtype=tf.string)\n",
        "  })\n",
        "\n",
        "  text_dataset = text_dataset_complex.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  return text_dataset\n",
        "\n",
        "def Skimlit(text):\n",
        "  text_dataset = preprocess_text(text)\n",
        "  predictions = model_5.predict(text_dataset)\n",
        "  predicted_classes = np.argmax(predictions, axis=1)\n",
        "  predicted_labels = [class_names[i] for i in predicted_classes]\n",
        "  result = dict(zip(split_sentence(text), predicted_labels))\n",
        "  result = pd.DataFrame(list(result.items()), columns=['Sentence', 'Label'])\n",
        "  return result\n",
        "\n",
        "\n",
        "Skimlit(abstract)"
      ],
      "metadata": {
        "id": "pH05QIJm3xUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_sentence(abstract)"
      ],
      "metadata": {
        "id": "rGTjErJ9IwO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test = \"\"\"This study investigated the safety and immunological impact of a third mRNA vaccine dose among immunocompromised adults in a multi-center cohort. Over a six-month period, more than one thousand participants from five tertiary hospitals were enrolled and followed after receiving a booster dose. Data were collected through patient-maintained diaries and weekly telehealth check-ins to track the incidence and severity of local and systemic adverse reactions. Blood samples were obtained before and after vaccination to measure immune response indicators, including IgG antibody titers, cytokine levels, and CD4/CD8 ratios. Participants included individuals with various immunosuppressive conditions such as cancer, organ transplants, and autoimmune disorders. Safety was assessed using standardized adverse event grading scales, while immunogenicity was analyzed through lab-based assays. The findings aim to inform public health guidelines on booster vaccination strategies for high-risk populations.\"\"\"\n",
        "Skimlit(Test)"
      ],
      "metadata": {
        "id": "UgI57Xy8OLtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_test = \"\"\"Hypertension continues to be a major contributor to cardiovascular disease, affecting nearly 1.3 billion individuals globally. Despite numerous therapeutic options, effective long-term blood pressure control remains suboptimal in many populations.\n",
        "This study aimed to evaluate the effectiveness of a daily low-dose combination of amlodipine and hydrochlorothiazide compared to monotherapy in achieving target blood pressure levels in adults aged 40–65 years with stage 1 hypertension.\n",
        "We conducted a 12-week, double-blind randomized clinical trial involving 480 participants across four urban medical centers. Patients were assigned to receive either the combination therapy or amlodipine alone. Blood pressure readings were taken weekly, and adverse effects were monitored via telehealth consultations and clinic visits.\n",
        "At the end of the trial, 72.3% of patients in the combination group achieved target systolic pressure, compared to 51.7% in the monotherapy group (p < 0.01). Reported side effects were mild and evenly distributed between groups.\n",
        "These findings suggest that low-dose combination therapy may provide superior outcomes in early-stage hypertension management, with minimal safety concerns and improved adherence potential.\"\"\"\n",
        "Skimlit(last_test)"
      ],
      "metadata": {
        "id": "3UzTh125PGhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define inputs for each branch\n",
        "token_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"token_inputs_embedding\")\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype=tf.float32, name=\"line_number_inputs\") # Corrected dtype and shape\n",
        "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=\"total_number_inputs\") # Corrected dtype and shape\n",
        "\n",
        "\n",
        "token_x = text_vectorizer(token_inputs)\n",
        "token_x = token_embed(token_x)\n",
        "token_x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_x)\n",
        "token_x = layers.GlobalMaxPool1D()(token_x)\n",
        "token_outputs = layers.Dense(5, activation=\"softmax\")(token_x)\n",
        "model = tf.keras.Model(token_inputs, token_outputs)\n",
        "\n",
        "# Token branch\n",
        "x = layers.Dense(128, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(64)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Line number branch: Dense layer\n",
        "line_number_output = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "\n",
        "# Total lines branch: Dense layer\n",
        "total_lines_output = layers.Dense(32, activation='relu')(total_lines_inputs)\n",
        "\n",
        "# Combine combined embeddings with positional embeddings\n",
        "tribrid_embeddings = layers.Concatenate(name=\"char_token_positional_embedding\")([line_number_output, total_lines_output, model.output])\n",
        "\n",
        "# Output layer\n",
        "output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(tribrid_embeddings)\n",
        "\n",
        "# Create the final model\n",
        "model_6 = tf.keras.Model(inputs=[line_number_inputs, total_lines_inputs, token_inputs],\n",
        "                         outputs=output_layer)\n",
        "\n",
        "model_6.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "1o_rVC8TQKvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import string\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "Num_char_token = len(alphabet) + 2\n",
        "Char_Vectorizer = TextVectorization(max_tokens=Num_char_token,\n",
        "                                    output_sequence_length=output_seq_len,\n",
        "                                    standardize=\"lower_and_strip_punctuation\")\n",
        "char_vocab = Char_Vectorizer.get_vocabulary()\n",
        "char_embed = layers.Embedding(input_dim=len(char_vocab),\n",
        "                              output_dim=25,\n",
        "                              mask_zero=True,\n",
        "                              name=\"char_embedding\")\n",
        "char_lens = [len(sentence) for sentence in train_sentences]\n",
        "output_seq_len = int(np.percentile(char_lens, 95))\n",
        "max_seq_len = max(char_lens)\n",
        "avg_sent = np.mean(char_lens)\n",
        "def split_char(text):\n",
        "  return \" \".join(list(text))\n",
        "\n",
        "train_chars = [split_char(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_char(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_char(sentence) for sentence in test_sentences]\n",
        "\n",
        "Char_Vectorizer.adapt(train_chars)"
      ],
      "metadata": {
        "id": "i0oAGzXscxLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Bidirectional, LSTM\n",
        "\n",
        "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_inputs\")\n",
        "char_vector = Char_Vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vector) # Assuming char_embed is defined as layers.Embedding\n",
        "char_bi_lstm = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings) # Set return_sequences=True and use_cudnn=False\n",
        "char_pooled = layers.GlobalMaxPool1D()(char_bi_lstm)\n",
        "\n",
        "# Output layer\n",
        "output = layers.Dense(5, activation='softmax')(char_pooled)\n",
        "model_6 = tf.keras.Model(char_inputs, outputs=output)\n",
        "model_6.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "model_6.fit(train_char_dataset, epochs=5, validation_data=(val_char_dataset))"
      ],
      "metadata": {
        "id": "MMYZiWT0UE7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQT13GG2U6jr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}